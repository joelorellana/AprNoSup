# TeorÃ­a para Live Coding: ReducciÃ³n Dimensional y Feature Engineering

## ğŸ“š GuÃ­a Completa de Conceptos TeÃ³ricos para MaestrÃ­a

---

## PARTE 1: FEATURE ENGINEERING (15 minutos)

### 1.1 ImputaciÃ³n de Missing Values

**Concepto clave:**
> Los datos reales siempre tienen valores faltantes. La forma de manejarlos puede hacer o romper tu modelo.

**Estrategias:**

**KNN Imputation (lo que usamos):**
```python
# Idea: "Dime quiÃ©n eres y te dirÃ© quÃ© valor falta"
# Encuentra K vecinos mÃ¡s similares
# Promedia sus valores (ponderado por distancia)
```

**Â¿Por quÃ© funciona?**
- Captura relaciones entre features
- Respeta estructura local
- MÃ¡s preciso que media/mediana

**Nuestra estrategia hÃ­brida:**
```
Missing < 30%  â†’ KNN (vale la pena la precisiÃ³n)
Missing > 30%  â†’ Median (KNN no confiable)
```

**ğŸ’¡ Pregunta:** *Â¿Por quÃ© no siempre KNN?*
- Con mucho missing, los "vecinos" no son confiables

---

### 1.2 Tratamiento de Outliers

**WinsorizaciÃ³n (lo que usamos):**
```python
# Reemplazar extremos con percentiles
x_new = clip(x, p01, p99)

Ventajas:
âœ… No perdemos muestras
âœ… Reducimos influencia de extremos
âœ… Preservamos orden relativo
```

**Alternativas:**
- **EliminaciÃ³n:** Pierdes datos
- **TransformaciÃ³n:** Log, sqrt comprime extremos

**ğŸ’¡ Pregunta:** *Â¿CuÃ¡ndo eliminar vs winsorizar?*
- Eliminar: Errores obvios, dataset grande
- Winsorizar: Valores extremos legÃ­timos

---

### 1.3 Transformaciones: Yeo-Johnson

**Â¿Por quÃ© transformar?**
- PCA asume normalidad (Gaussiana)
- Mejora convergencia
- Reduce impacto de outliers

**Yeo-Johnson vs Box-Cox:**
```python
Box-Cox:  Solo valores POSITIVOS
Yeo-Johnson: Funciona con NEGATIVOS âœ…

# Datos de proteÃ­nas pueden tener negativos
# (log-ratios, z-scores)
```

---

### 1.4 Feature Engineering Creativo

**A. Features EstadÃ­sticos:**
```python
mean_expression    # Nivel promedio
std_expression     # Variabilidad
cv_expression      # Variabilidad relativa (std/mean)
skewness          # AsimetrÃ­a de distribuciÃ³n
kurtosis          # "Peakedness"
```

**Â¿Por quÃ© funcionan?**
- Capturan patrones globales por muestra
- Reducen ruido
- Pueden ser mÃ¡s discriminativos

**Ejemplo biolÃ³gico:**
```
Muestra A: mean=10, cv=0.1  â†’ ExpresiÃ³n estable
Muestra B: mean=10, cv=0.8  â†’ ExpresiÃ³n errÃ¡tica
â†’ Mismo mean, diferente biologÃ­a
```

**B. Features de InteracciÃ³n:**
```python
# Ratios
ratio = protein_A / protein_B
# Captura balance entre procesos

# Productos
product = protein_A Ã— protein_B
# Captura co-ocurrencia
```

**Â¿CÃ³mo seleccionar pares?**
- CorrelaciÃ³n alta (>0.7)
- Domain knowledge
- Feature importance

**âš ï¸ Cuidado:** 77 features â†’ 2,926 pares posibles
â†’ Usamos solo top 10

---

### 1.5 Escalado: RobustScaler

**Â¿Por quÃ© escalar?**
```
Feature A: [0, 1]
Feature B: [0, 1000]
â†’ B domina distancias
â†’ Algoritmos sesgados
```

**RobustScaler:**
```python
x_new = (x - median) / IQR

Ventajas:
âœ… Robusto a outliers (usa percentiles)
âœ… Segunda lÃ­nea de defensa despuÃ©s de winsorizaciÃ³n
```

---

## PARTE 2: PCA (20 minutos)

### 2.1 IntuiciÃ³n

**AnalogÃ­a:**
> Fotografiar un objeto 3D desde el mejor Ã¡ngulo que captura mÃ¡s informaciÃ³n

**Concepto:**
- Encontrar direcciones de mÃ¡xima varianza
- Proyectar datos en esas direcciones
- Reducir dimensionalidad manteniendo informaciÃ³n

### 2.2 MatemÃ¡ticas (nivel maestrÃ­a)

**Pasos:**
```python
1. Centrar datos: X_centered = X - mean
2. Matriz covarianza: Î£ = X.T @ X / n
3. EigendecomposiciÃ³n: Î£v = Î»v
4. Ordenar por eigenvalues: Î»â‚ â‰¥ Î»â‚‚ â‰¥ ...
```

**Propiedades:**
- **Ortogonalidad:** PCs no correlacionados
- **Ordenamiento:** PC1 captura mÃ¡s varianza
- **ReconstrucciÃ³n:** Minimiza error de reconstrucciÃ³n

### 2.3 InterpretaciÃ³n

**Scree Plot:**
```
Buscar "codo" donde curva se aplana
Antes del codo: componentes importantes
DespuÃ©s: ruido
```

**Reglas de selecciÃ³n:**
```
1. Regla del codo (visual)
2. Varianza acumulada (80%, 90%, 95%)
3. Eigenvalue > 1 (Kaiser)
4. ValidaciÃ³n cruzada (mejor)
```

**Loadings:**
```python
Loading[i,j] = correlaciÃ³n entre feature i y PC j

|loading| alto â†’ feature importante
loading > 0 â†’ contribuciÃ³n positiva
loading < 0 â†’ contribuciÃ³n negativa
```

**Ejemplo:**
```
PC1 loadings:
  ProteÃ­na_A:  0.85  â† Alta contribuciÃ³n +
  ProteÃ­na_D: -0.65  â† Alta contribuciÃ³n -

InterpretaciÃ³n:
PC1 = "Eje de A vs D"
Score alto â†’ A alto, D bajo
```

### 2.4 Ventajas y Limitaciones

**âœ… Ventajas:**
- DeterminÃ­stico (reproducible)
- RÃ¡pido (escalable)
- Interpretable (loadings)
- Reduce ruido
- Decorrelaciona features

**âŒ Limitaciones:**
- Solo lineal
- Sensible a escala (escalar primero)
- Sensible a outliers
- MÃ¡xima varianza â‰  mÃ¡xima discriminaciÃ³n

**ğŸ’¡ Pregunta:** *Â¿Por quÃ© PCA no siempre separa clases?*
- Varianza y discriminaciÃ³n son diferentes
- Puede haber mucha varianza dentro de clases

---

## PARTE 3: t-SNE (20 minutos)

### 3.1 MotivaciÃ³n

**Problema con PCA:**
```
Datos no-lineales (espiral, Swiss roll)
PCA proyecta linealmente â†’ pierde estructura
```

**Objetivo t-SNE:**
> Preservar estructura LOCAL: puntos cercanos deben permanecer cercanos

### 3.2 Algoritmo (intuiciÃ³n)

**Idea:**
```
1. Calcular similitudes en alta dimensiÃ³n (Gaussiana)
2. Calcular similitudes en baja dimensiÃ³n (t-Student)
3. Optimizar para que coincidan (minimizar KL divergence)
```

**Â¿Por quÃ© distribuciÃ³n t-Student?**
- Colas mÃ¡s pesadas que Gaussiana
- Permite separar puntos moderadamente lejanos
- Resuelve "crowding problem"

### 3.3 HiperparÃ¡metro: Perplexity

**DefiniciÃ³n:**
```
Perplexity â‰ˆ "nÃºmero efectivo de vecinos"
Perplexity = 30 â†’ considera ~30 vecinos
```

**Efectos:**
```
Perplexity BAJO (5-10):
âœ… Estructura muy local
âŒ Sobre-fragmenta clusters

Perplexity MEDIO (30-50):
âœ… Balance local-global
âœ… Recomendado

Perplexity ALTO (100+):
âœ… MÃ¡s global
âŒ Fusiona clusters pequeÃ±os
```

**Regla prÃ¡ctica:**
```
n < 100:      perplexity = 5-15
n = 100-1000: perplexity = 30-50
n > 1000:     perplexity = 50-100
```

### 3.4 InterpretaciÃ³n

**âœ… SÃ puedes interpretar:**
- Proximidad local (puntos cercanos son similares)
- SeparaciÃ³n de clusters
- Estructura interna de clusters

**âŒ NO puedes interpretar:**
- Distancias globales entre clusters
- TamaÃ±o de clusters
- Densidades
- Distancias numÃ©ricas

**Ejemplo de mala interpretaciÃ³n:**
```
âŒ "Cluster A estÃ¡ 2x mÃ¡s lejos de C que B"
âœ… "Clusters A y B estÃ¡n separados de C"
```

### 3.5 Ventajas y Limitaciones

**âœ… Ventajas:**
- Excelente visualizaciÃ³n
- Captura no-linealidad
- Clusters bien definidos

**âŒ Limitaciones:**
- NO para ML pipelines (no determinÃ­stico)
- Lento O(nÂ²)
- Sensible a hiperparÃ¡metros
- No preserva estructura global
- Puede crear estructura artificial

**ğŸ’¡ Pregunta:** *Â¿Por quÃ© no usar t-SNE en pipelines ML?*
- No determinÃ­stico
- No hay transform() para nuevos datos
- Cada aplicaciÃ³n requiere re-optimizaciÃ³n

---

## PARTE 4: UMAP (20 minutos)

### 4.1 MotivaciÃ³n

**Mejoras sobre t-SNE:**
```
âœ… MÃ¡s rÃ¡pido O(n log n)
âœ… Escalable
âœ… Preserva estructura global
âœ… DeterminÃ­stico (con seed)
âœ… Puede usarse en ML pipelines
```

### 4.2 Algoritmo (intuiciÃ³n)

**Base teÃ³rica:**
- TeorÃ­a de variedades de Riemannian
- TopologÃ­a algebraica
- Idea: datos viven en variedad de baja dimensiÃ³n

**Pasos:**
```
1. Construir grafo fuzzy en alta dimensiÃ³n
   - Encontrar k vecinos
   - Distancias adaptativas locales
   
2. Optimizar embedding en baja dimensiÃ³n
   - Minimizar cross-entropy
   - Preservar topologÃ­a
```

### 4.3 HiperparÃ¡metros

**n_neighbors:**
```
n_neighbors BAJO (5-10):
âœ… Estructura muy local
âœ… Clusters detallados
âŒ Puede fragmentar

n_neighbors MEDIO (15-30):
âœ… Balance recomendado
âœ… Buena separaciÃ³n

n_neighbors ALTO (50+):
âœ… MÃ¡s global
âœ… Estructura suave
âŒ Pierde detalles
```

**min_dist:**
```
min_dist = 0.0:  Clusters muy compactos
min_dist = 0.1:  Balance (recomendado)
min_dist = 0.5:  Clusters mÃ¡s dispersos
```

**Regla prÃ¡ctica:**
```
ExploraciÃ³n inicial: n_neighbors=15, min_dist=0.1
Ajustar segÃºn necesidad
```

### 4.4 UMAP vs t-SNE

**ComparaciÃ³n:**
```
| Aspecto           | t-SNE      | UMAP        |
|-------------------|------------|-------------|
| Velocidad         | O(nÂ²)      | O(n log n)  |
| Escalabilidad     | <10k       | Millones    |
| Determinismo      | No         | SÃ­ (seed)   |
| Estructura global | Pobre      | Buena       |
| Estructura local  | Excelente  | Excelente   |
| ML pipeline       | âŒ         | âœ…          |
| InterpretaciÃ³n    | Cualitativa| Semi-cuant  |
```

### 4.5 Ventajas y Limitaciones

**âœ… Ventajas:**
- RÃ¡pido y escalable
- Balance local-global
- DeterminÃ­stico
- Transform para nuevos datos
- Puede usarse en ML
- Preserva mÃ¡s estructura que t-SNE

**âŒ Limitaciones:**
- Menos maduro que PCA/t-SNE
- TeorÃ­a matemÃ¡tica compleja
- HiperparÃ¡metros menos intuitivos
- Puede sobre-optimizar en datos pequeÃ±os

---

## PARTE 5: COMPARACIÃ“N Y MÃ‰TRICAS (15 minutos)

### 5.1 MÃ©tricas de Clustering

**Silhouette Score [-1, 1]:**
```python
s = (b - a) / max(a, b)

a = distancia promedio intra-cluster
b = distancia promedio al cluster mÃ¡s cercano

InterpretaciÃ³n:
s â‰ˆ 1:  Bien separado
s â‰ˆ 0:  En frontera
s < 0:  Mal asignado
```

**Calinski-Harabasz (mayor mejor):**
```python
CH = (SSB / SSW) Ã— ((n - k) / (k - 1))

SSB = suma de cuadrados entre clusters
SSW = suma de cuadrados dentro clusters

InterpretaciÃ³n:
Alto â†’ clusters compactos y separados
```

**Davies-Bouldin (menor mejor):**
```python
DB = (1/k) Î£ max((Ïƒáµ¢ + Ïƒâ±¼) / d(cáµ¢, câ±¼))

InterpretaciÃ³n:
Bajo â†’ clusters compactos y lejanos
```

### 5.2 InterpretaciÃ³n de Resultados

**Ejemplo de nuestro dataset:**
```
         Method  Silhouette  Calinski-H  Davies-B
            PCA      -0.065        91.4       7.20
t-SNE (perp=30)      -0.007       231.6       5.29
    UMAP (n=15)      -0.005       313.5       6.08
```

**AnÃ¡lisis:**

**Silhouette (todos negativos):**
- Normal en datos biolÃ³gicos
- Overlap natural entre clases
- UMAP mejor (menos negativo)

**Calinski-Harabasz:**
- UMAP: 313.5 (mejor) âœ¨
- 3.4x mejor que PCA
- Mejor separaciÃ³n global

**Davies-Bouldin:**
- t-SNE: 5.29 (mejor) âœ¨
- Clusters mÃ¡s compactos
- Mejor estructura local

**ConclusiÃ³n:**
- UMAP: Mejor balance general
- t-SNE: Mejor compactness local
- PCA: Limitado por linealidad

### 5.3 CuÃ¡ndo Usar Cada TÃ©cnica

**PCA:**
```
âœ… AnÃ¡lisis exploratorio rÃ¡pido
âœ… ReducciÃ³n para ML pipeline
âœ… Interpretabilidad (loadings)
âœ… Datasets con relaciones lineales
âœ… Cuando necesitas determinismo
```

**t-SNE:**
```
âœ… VisualizaciÃ³n para papers
âœ… ExploraciÃ³n de clusters
âœ… DetecciÃ³n de estructura local
âœ… Presentaciones
âŒ NO para ML pipelines
âŒ NO para datos >10k puntos
```

**UMAP:**
```
âœ… Balance local-global
âœ… ML pipelines (con transform)
âœ… Datasets grandes (escalable)
âœ… Cuando necesitas velocidad
âœ… PreservaciÃ³n de topologÃ­a
âœ… Alternativa moderna a t-SNE
```

### 5.4 Workflow Recomendado

**ExploraciÃ³n inicial:**
```
1. PCA primero (rÃ¡pido, interpretable)
   - Ver varianza explicada
   - Identificar outliers
   - Entender features importantes

2. t-SNE para visualizaciÃ³n
   - Probar mÃºltiples perplexities
   - Identificar clusters
   - Validar con domain knowledge

3. UMAP para confirmaciÃ³n
   - Comparar con t-SNE
   - Verificar estructura global
   - Usar en pipeline si necesario
```

**Para producciÃ³n:**
```
1. PCA si relaciones lineales
2. UMAP si estructura compleja
3. Nunca t-SNE (no reproducible)
```

---

## CONCEPTOS AVANZADOS (Opcional)

### Kernel PCA
```
Idea: PCA en espacio de features no-lineal
Kernel: rbf, poly, sigmoid
Captura no-linealidad sin manifold learning
```

### Autoencoders
```
Red neuronal para reducciÃ³n dimensional
Encoder: alta-D â†’ baja-D
Decoder: baja-D â†’ alta-D
MÃ¡s flexible que PCA
```

### ComparaciÃ³n TeÃ³rica

**PreservaciÃ³n:**
```
PCA:   Preserva varianza global
t-SNE: Preserva vecindades locales
UMAP:  Preserva topologÃ­a (local + global)
```

**Complejidad:**
```
PCA:   O(min(nÂ²p, npÂ²))
t-SNE: O(nÂ² log n) con Barnes-Hut
UMAP:  O(n log n)
```

**Fundamento:**
```
PCA:   Ãlgebra lineal (eigenvectors)
t-SNE: Probabilidad (KL divergence)
UMAP:  TopologÃ­a (Riemannian manifolds)
```

---

## PREGUNTAS FRECUENTES

**P: Â¿Siempre escalar antes de PCA?**
R: SÃ. PCA es sensible a escala. Features con mayor varianza dominan.

**P: Â¿CuÃ¡ntos componentes retener?**
R: Depende. Regla general: 80-95% varianza. Mejor: validaciÃ³n cruzada.

**P: Â¿Por quÃ© t-SNE da resultados diferentes?**
R: InicializaciÃ³n aleatoria + optimizaciÃ³n estocÃ¡stica. Fijar random_state.

**P: Â¿UMAP siempre mejor que t-SNE?**
R: No siempre. t-SNE puede ser mejor para estructura muy local. UMAP mejor para balance.

**P: Â¿Puedo usar t-SNE para clasificaciÃ³n?**
R: NO. No hay transform(). Solo visualizaciÃ³n.

**P: Â¿Reducir con PCA antes de t-SNE?**
R: SÃ, recomendado. Reduce a ~50 dims primero. MÃ¡s rÃ¡pido, usualmente sin pÃ©rdida.

**P: Â¿CÃ³mo validar que reducciÃ³n es buena?**
R: 
1. MÃ©tricas de clustering
2. ClasificaciÃ³n downstream
3. ValidaciÃ³n con domain knowledge
4. MÃºltiples tÃ©cnicas (triangulaciÃ³n)

---

## MENSAJES CLAVE PARA ESTUDIANTES

1. **Feature Engineering es crucial**
   - Puede mejorar modelo mÃ¡s que algoritmo
   - Requiere domain knowledge
   - Iterativo: experimentar y validar

2. **No hay tÃ©cnica "mejor"**
   - Depende de objetivo y datos
   - PCA: rÃ¡pido, interpretable, lineal
   - t-SNE: visualizaciÃ³n, local
   - UMAP: balance, escalable

3. **Siempre validar**
   - MÃ©tricas cuantitativas
   - ValidaciÃ³n cruzada
   - Domain knowledge
   - MÃºltiples tÃ©cnicas

4. **Entender limitaciones**
   - PCA: solo lineal
   - t-SNE: no para ML, no global
   - UMAP: menos maduro

5. **Workflow importa**
   - Explorar â†’ Validar â†’ ProducciÃ³n
   - Documentar decisiones
   - Reproducibilidad

---

## RECURSOS ADICIONALES

**Papers:**
- PCA: Pearson (1901)
- t-SNE: van der Maaten & Hinton (2008)
- UMAP: McInnes et al. (2018)

**Tutoriales:**
- Distill.pub: "How to Use t-SNE Effectively"
- UMAP docs: https://umap-learn.readthedocs.io/

**Libros:**
- "Feature Engineering for ML" - Alice Zheng
- "Hands-On ML" - AurÃ©lien GÃ©ron

---

**Â¡Fin de la teorÃ­a! Ahora a codear! ğŸš€**
