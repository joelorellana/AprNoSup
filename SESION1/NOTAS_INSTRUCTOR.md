# Notas para el Instructor - Live Coding Session

## ğŸ“‹ Resumen de la SesiÃ³n (90 minutos)

### Dataset: Mice Protein Expression
- **Fuente:** UCI Machine Learning Repository
- **CaracterÃ­sticas:** 1080 muestras, 77 proteÃ­nas, 8 clases
- **Por quÃ© este dataset:**
  - Poco conocido (no es Iris, MNIST, etc.)
  - Datos reales con problemas reales (missing values, outliers)
  - Suficientemente complejo para maestrÃ­a
  - BiolÃ³gicamente relevante

---

## â±ï¸ Timing Sugerido

### Bloque 1: ExploraciÃ³n y Feature Engineering (15 min)
**Minutos 0-15**

**Puntos clave a enfatizar:**
1. **ImputaciÃ³n estratificada** (KNN vs median)
   - Explicar por quÃ© KNN para <30% missing
   - Discutir trade-offs: precisiÃ³n vs complejidad

2. **WinsorizaciÃ³n vs eliminaciÃ³n de outliers**
   - Mostrar que no perdemos datos
   - Preservamos informaciÃ³n pero limitamos extremos

3. **Transformaciones Yeo-Johnson**
   - Por quÃ© no Box-Cox (valores negativos)
   - Importancia de normalidad para PCA

4. **Feature Engineering creativo:**
   - Features estadÃ­sticos (mean, std, cv, skew, kurtosis)
   - Ratios y productos de proteÃ­nas correlacionadas
   - Explicar el concepto de "domain knowledge"

**ğŸ’¡ Pregunta para estudiantes:**
*"Â¿Por quÃ© crear features de interacciÃ³n entre proteÃ­nas correlacionadas?"*
- Respuesta: Capturar relaciones no lineales que mÃ©todos lineales (PCA) no ven

---

### Bloque 2: PCA (20 min)
**Minutos 15-35**

**DemostraciÃ³n paso a paso:**

1. **Scree Plot** (5 min)
   - Explicar "codo" (elbow method)
   - Discutir trade-off: varianza vs dimensiones
   - Mostrar que 90% varianza â‰  90% informaciÃ³n Ãºtil

2. **VisualizaciÃ³n 2D** (5 min)
   - Interpretar separaciÃ³n de clases
   - Explicar por quÃ© algunas clases se solapan
   - Limitaciones de proyecciÃ³n lineal

3. **AnÃ¡lisis de Loadings** (10 min)
   - **MUY IMPORTANTE:** Explicar quÃ© son los loadings
   - Mostrar features que mÃ¡s contribuyen a PC1 y PC2
   - Conectar con biologÃ­a (si es posible)
   
**ğŸ’¡ Pregunta para estudiantes:**
*"Si PC1 explica 40% de varianza, Â¿significa que es la dimensiÃ³n mÃ¡s importante para clasificaciÃ³n?"*
- Respuesta: NO necesariamente. Varianza â‰  discriminaciÃ³n

**âš ï¸ Advertencias comunes:**
- PCA asume linealidad
- Sensible a escala (por eso escalamos)
- No garantiza separabilidad de clases

---

### Bloque 3: t-SNE (20 min)
**Minutos 35-55**

**Experimentos con perplexity:**

1. **Perplexity = 5** (estructura muy local)
   - Muchos clusters pequeÃ±os
   - Puede sobre-fragmentar

2. **Perplexity = 30** (recomendado)
   - Balance tÃ­pico
   - Buena separaciÃ³n

3. **Perplexity = 50-100** (mÃ¡s global)
   - Clusters mÃ¡s grandes
   - Puede perder detalles

**ğŸ’¡ Pregunta para estudiantes:**
*"Â¿Por quÃ© t-SNE produce resultados diferentes cada vez?"*
- Respuesta: OptimizaciÃ³n estocÃ¡stica, inicializaciÃ³n aleatoria

**âš ï¸ Advertencias CRÃTICAS:**
- **NO usar para reducciÃ³n dimensional en pipelines ML**
- **NO interpretar distancias globales**
- **NO comparar densidades entre clusters**
- Computacionalmente costoso (O(nÂ²))

**Conceptos avanzados a mencionar:**
- t-SNE preserva probabilidades de vecindad
- Usa distribuciÃ³n t de Student (colas pesadas)
- Crowding problem y cÃ³mo t-SNE lo resuelve

---

### Bloque 4: UMAP (20 min)
**Minutos 55-75**

**Experimentos con n_neighbors:**

1. **n_neighbors = 5** (muy local)
   - Similar a t-SNE con perplexity baja
   - Estructura granular

2. **n_neighbors = 15** (recomendado)
   - Balance Ã³ptimo tÃ­pico
   - Buena separaciÃ³n

3. **n_neighbors = 30-50** (mÃ¡s global)
   - Estructura mÃ¡s suave
   - Mejor preservaciÃ³n global

**ğŸ’¡ Pregunta para estudiantes:**
*"Â¿CuÃ¡l es la principal ventaja de UMAP sobre t-SNE?"*
- Respuestas:
  1. MÃ¡s rÃ¡pido (puede escalar a millones de puntos)
  2. Preserva mejor estructura global
  3. DeterminÃ­stico (con mismo random_state)
  4. Puede usarse en pipelines ML

**Conceptos avanzados:**
- TeorÃ­a de variedades de Riemannian
- PreservaciÃ³n de topologÃ­a
- min_dist vs n_neighbors

**ComparaciÃ³n directa t-SNE vs UMAP:**
| Aspecto | t-SNE | UMAP |
|---------|-------|------|
| Velocidad | Lento (O(nÂ²)) | RÃ¡pido (O(n log n)) |
| Determinismo | No | SÃ­ (con seed) |
| Estructura global | Pobre | Buena |
| Estructura local | Excelente | Excelente |
| Uso en ML | âŒ | âœ… |

---

### Bloque 5: ComparaciÃ³n Cuantitativa (15 min)
**Minutos 75-90**

**MÃ©tricas de clustering:**

1. **Silhouette Score** (-1 a 1, mayor mejor)
   - Mide cohesiÃ³n y separaciÃ³n
   - FÃ¡cil de interpretar

2. **Calinski-Harabasz** (mayor mejor)
   - Ratio de dispersiÃ³n entre/dentro clusters
   - Favorece clusters compactos y separados

3. **Davies-Bouldin** (menor mejor)
   - Promedio de similitud entre clusters
   - Penaliza clusters cercanos

**ğŸ’¡ Pregunta para estudiantes:**
*"Â¿Por quÃ© UMAP suele tener mejores mÃ©tricas que t-SNE?"*
- Respuesta: Mejor preservaciÃ³n de distancias globales

**VisualizaciÃ³n comparativa:**
- Mostrar las 3 tÃ©cnicas lado a lado
- Discutir trade-offs
- No hay "ganador" absoluto

---

## ğŸ¯ Puntos Clave de Feature Engineering

### 1. ImputaciÃ³n
```python
# Estrategia hÃ­brida
- KNN: Para missing < 30% (mÃ¡s preciso)
- Median: Para missing > 30% (mÃ¡s robusto)
```

### 2. Outliers
```python
# WinsorizaciÃ³n vs EliminaciÃ³n
- WinsorizaciÃ³n: Preserva muestras, limita extremos
- EliminaciÃ³n: Pierde informaciÃ³n
```

### 3. Transformaciones
```python
# Yeo-Johnson vs Box-Cox
- Yeo-Johnson: Funciona con negativos
- Box-Cox: Solo positivos
```

### 4. Features EstadÃ­sticos
```python
# Por muestra (axis=1)
- mean, std, cv (coef. variaciÃ³n)
- skewness, kurtosis
- percentiles (q25, q75, IQR)
```

### 5. Features de InteracciÃ³n
```python
# Entre features correlacionadas
- Ratios: f1 / f2
- Productos: f1 * f2
- Diferencias: f1 - f2
```

---

## ğŸ“ Preguntas Avanzadas para DiscusiÃ³n

### Nivel 1: Conceptual
1. Â¿Por quÃ© PCA no garantiza buena separaciÃ³n de clases?
2. Â¿CuÃ¡ndo preferirÃ­as t-SNE sobre UMAP?
3. Â¿CÃ³mo afecta el escalado a cada tÃ©cnica?

### Nivel 2: PrÃ¡ctico
1. Â¿CÃ³mo seleccionarÃ­as el nÃºmero de componentes en PCA?
2. Â¿CÃ³mo validarÃ­as que tu feature engineering mejorÃ³ el modelo?
3. Â¿QuÃ© harÃ­as si t-SNE muestra clusters que PCA no muestra?

### Nivel 3: Avanzado
1. Â¿CÃ³mo implementarÃ­as UMAP en un pipeline de producciÃ³n?
2. Â¿QuÃ© tÃ©cnicas usarÃ­as para datasets con millones de muestras?
3. Â¿CÃ³mo combinarÃ­as feature engineering con deep learning?

---

## ğŸ”§ Troubleshooting ComÃºn

### Problema 1: "t-SNE tarda mucho"
**SoluciÃ³n:**
- Reducir primero con PCA a ~50 dimensiones
- Usar menos iteraciones (500 en vez de 1000)
- Considerar usar UMAP

### Problema 2: "UMAP da resultados extraÃ±os"
**SoluciÃ³n:**
- Ajustar n_neighbors (probar 5, 15, 30, 50)
- Ajustar min_dist (0.0 a 0.99)
- Verificar escalado de datos

### Problema 3: "PCA no separa clases"
**SoluciÃ³n:**
- Verificar que datos estÃ©n escalados
- Probar transformaciones (log, sqrt, Box-Cox)
- Considerar PCA no lineal (Kernel PCA)

### Problema 4: "Muchos missing values"
**SoluciÃ³n:**
- Evaluar si eliminar features con >50% missing
- Usar imputaciÃ³n mÃºltiple
- Crear feature binario "was_missing"

---

## ğŸ“š Referencias para Profundizar

### Papers Fundamentales:
1. **PCA:** Pearson (1901) - Original paper
2. **t-SNE:** van der Maaten & Hinton (2008)
3. **UMAP:** McInnes et al. (2018)

### Recursos Online:
- Distill.pub: "How to Use t-SNE Effectively"
- UMAP documentation: https://umap-learn.readthedocs.io/
- Scikit-learn User Guide: Manifold Learning

### Libros:
- "Feature Engineering for Machine Learning" - Alice Zheng
- "Hands-On Machine Learning" - AurÃ©lien GÃ©ron (Cap. 8)

---

## ğŸ’» Extensiones Opcionales (si sobra tiempo)

### 1. Kernel PCA
```python
from sklearn.decomposition import KernelPCA
kpca = KernelPCA(n_components=2, kernel='rbf')
X_kpca = kpca.fit_transform(X_scaled)
```

### 2. Autoencoder (reducciÃ³n dimensional con DL)
```python
# Arquitectura simple
# Input -> Dense(64) -> Dense(32) -> Dense(2) -> Dense(32) -> Dense(64) -> Output
```

### 3. AnÃ¡lisis de estabilidad
```python
# Ejecutar t-SNE mÃºltiples veces
# Medir variabilidad de resultados
```

### 4. Feature Selection
```python
from sklearn.feature_selection import SelectKBest, f_classif
# Comparar con/sin feature selection
```

---

## âœ… Checklist Pre-SesiÃ³n

- [ ] Verificar que todas las librerÃ­as estÃ©n instaladas
- [ ] Probar descarga del dataset (puede fallar)
- [ ] Tener dataset local como backup
- [ ] Ejecutar notebook completo una vez
- [ ] Preparar respuestas a preguntas comunes
- [ ] Tener ejemplos adicionales listos

---

## ğŸ¯ Objetivos de Aprendizaje

Al final de la sesiÃ³n, los estudiantes deben poder:

1. âœ… Aplicar feature engineering avanzado a datos reales
2. âœ… Entender diferencias conceptuales entre PCA, t-SNE y UMAP
3. âœ… Seleccionar la tÃ©cnica apropiada segÃºn el contexto
4. âœ… Interpretar visualizaciones de reducciÃ³n dimensional
5. âœ… Evaluar calidad de embeddings con mÃ©tricas
6. âœ… Identificar limitaciones de cada tÃ©cnica

---

## ğŸ“ Notas Finales

**Enfoque pedagÃ³gico:**
- MÃ¡s prÃ¡ctica que teorÃ­a
- Enfatizar intuiciÃ³n sobre matemÃ¡ticas
- Usar visualizaciones constantemente
- Fomentar experimentaciÃ³n

**Mensaje final para estudiantes:**
> "No existe una tÃ©cnica 'mejor'. La elecciÃ³n depende de:
> - Objetivo (visualizaciÃ³n vs ML pipeline)
> - TamaÃ±o de datos
> - Estructura esperada (local vs global)
> - Recursos computacionales
> - Necesidad de reproducibilidad"

Â¡Buena suerte con la sesiÃ³n! ğŸš€
